{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76e035",
   "metadata": {},
   "source": [
    "## Preparing the data for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69433ad3",
   "metadata": {},
   "source": [
    "### Pipeline 1: Collecting and preparing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567fef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name for file management\n",
    "graph_name = \"Marketing\"\n",
    "\n",
    "ufilename = graph_name + \"_urls.txt\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "with open(ufilename, 'r') as file:\n",
    "    urls = [line.strip() for line in file]\n",
    "\n",
    "print(\"Read URLs:\")\n",
    "for url in urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(content):\n",
    "    # Remove references and unwanted characters\n",
    "    content = re.sub(r'\\[\\d+\\]', '', content)   # Remove references\n",
    "    content = re.sub(r'[^\\w\\s\\.]', '', content)  # Remove punctuation (except periods)\n",
    "    return content\n",
    "\n",
    "def safe_file_name(s):\n",
    "    # Replace spaces with underscores\n",
    "    s = s.replace(' ', '_')\n",
    "    \n",
    "    # Remove any characters that are not allowed in file names\n",
    "    safe_str = ''.join(c for c in s if c.isalpha() or c.isdigit() or c in [' ', '.', '_', '-'])\n",
    "    \n",
    "    return safe_str\n",
    "\n",
    "def file_exists_and_has_content(file_path):\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        return False\n",
    "    \n",
    "    # Check if the file is not empty\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        first_char = file.read(1)\n",
    "        if first_char:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def fetch_and_clean(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Prioritise \"mw-parser-output\" but fall back to \"content\" node if not found\n",
    "        content = soup.find('div', {'class': 'mw-parser-output'}) or soup.find('div', {'id': 'content'})\n",
    "        if content is None:\n",
    "            return None\n",
    "        \n",
    "        # Remove specific unwanted sections, including nested ones\n",
    "        for section_title in ['References', 'Bibliography', 'External links', 'See also', 'Notes']:\n",
    "            section = content.find('span', id=section_title)\n",
    "            while section:\n",
    "                for sib in section.parent.find_next_siblings():\n",
    "                    sib.decompose()  # Remove the section and its siblings\n",
    "                section.parent.decompose()  # Remove the section itself\n",
    "                section = content.find('span', id=section_title)\n",
    "\n",
    "        # Extract and clean text\n",
    "        text = content.get_text(separator=' ', strip=True)  # Use space as separator and strip whitespace\n",
    "        text = clean_text(text)\n",
    "        return text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None  # Return None if there's an error\n",
    "    \n",
    "# Directory to store the output file\n",
    "output_dir = './data/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Processing the URLs and skipping invalid ones\n",
    "reload = False  # Set to True to reprocess all URLs\n",
    "if reload==True:\n",
    "    for url in urls:\n",
    "        article_name = url.split('/')[-1].replace('.html', '')\n",
    "        filename = os.path.join(output_dir, f\"{safe_file_name(article_name)}.txt\")\n",
    "\n",
    "        if file_exists_and_has_content(filename) is True:\n",
    "            print(f\"Existed {filename}\")\n",
    "            continue\n",
    "        else:\n",
    "            clean_article_text = fetch_and_clean(url)\n",
    "            if clean_article_text:  # Only write if text is not None\n",
    "                with open(filename, 'w', encoding='utf-8') as file:\n",
    "                    file.write(clean_article_text)\n",
    "                    print(f\"Saved {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a539dac",
   "metadata": {},
   "source": [
    "## Pipeline 2: Creating and populating the Deeplake Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fddd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup embedding model\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "# # MistralAI embedding\n",
    "# # rate limit of 1 request per second, set a large batch size to avoid rate limiting...\n",
    "# from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "# embedding_model_name = \"mistral-embed\"\n",
    "# embed_model = MistralAIEmbedding(\n",
    "#     model_name = embedding_model_name,\n",
    "#     api_key = os.getenv(\"MISTRAL_API_KEY\"),\n",
    "#     embed_batch_size = 30\n",
    "# )\n",
    "\n",
    "\n",
    "# Using local embedding models served by LM Studio\n",
    "# Use fake API key (LM Studio doesn't validate it)\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "embed_model = OpenAIEmbedding(\n",
    "    api_base = os.getenv(\"LM_STUDIO_API_BASE\"),\n",
    "    api_key = \"whatever-is-in-lmstudio\",\n",
    "    model_name = os.getenv(\"LM_STUDIO_EMBEDDING_MODEL\"),\n",
    "    embed_batch_size = 10\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Embedding model verification\n",
    "Settings.embed_model = embed_model\n",
    "embed = embed_model.get_text_embedding(\"The quick brown fox jumps over the lazy dog.\")\n",
    "print(embed[:5])  # Should print a list of floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5244261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "\n",
    "# Path for vector store and dataset\n",
    "# os.environ['ACTIVELOOP_TOKEN'] = os.getenv('ACTIVELOOP_TOKEN')\n",
    "# database = \"hub://honglin/marketing01\" # hosted deeplake database\n",
    "database = \"./dataset/marketing01\" # local storage\n",
    "vector_store_path = database\n",
    "dataset_path = database\n",
    "\n",
    "# Create an index over the documents\n",
    "# Overwrites the existing dataset if True\n",
    "ow = False\n",
    "\n",
    "if ow==True:\n",
    "    try:\n",
    "        vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=ow)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        index = VectorStoreIndex.from_documents(documents, storage_context, show_progress=True, embed_model=Settings.embed_model)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"Error type: {type(e)}\")\n",
    "        print(f\"Error traceback: {e.__traceback__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc46409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n",
    "ds = deeplake.load(dataset_path)\n",
    "ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d73ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a dictionary to hold the data\n",
    "data = {}\n",
    "\n",
    "# Iterate through the tensors in the dataset\n",
    "for tensor_name in ds.tensors:\n",
    "    tensor_data = ds[tensor_name].numpy()\n",
    "\n",
    "    # Check if the tensor is multi-dimensional\n",
    "    if tensor_data.ndim > 1:\n",
    "        # Flatten multi-dimensional tensors\n",
    "        data[tensor_name] = [np.array(e).flatten().tolist() for e in tensor_data]\n",
    "    else:\n",
    "        # Convert 1D tensors directly to lists and decode text\n",
    "        if tensor_name == \"text\":\n",
    "            data[tensor_name] = [t.tobytes().decode('utf-8') if t else \"\" for t in tensor_data]\n",
    "        else:\n",
    "            data[tensor_name] = tensor_data.tolist()\n",
    "\n",
    "# Create a Pandas DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display a selected record\n",
    "def display_record(record_number):\n",
    "    record = df.iloc[record_number]\n",
    "    display_data = {\n",
    "        \"ID\": record.get(\"id\", \"N/A\"),\n",
    "        \"Metadata\": record.get(\"metadata\", \"N/A\"),\n",
    "        \"Text\": record.get(\"text\", \"N/A\"),\n",
    "        \"Embedding\": record.get(\"embedding\", \"N/A\")\n",
    "    }\n",
    "\n",
    "    # Print the ID\n",
    "    print(\"ID:\")\n",
    "    print(display_data[\"ID\"])\n",
    "    print()\n",
    "\n",
    "    # Print the metadata in a structured format\n",
    "    print(\"Metadata:\")\n",
    "    metadata = display_data[\"Metadata\"]\n",
    "    if isinstance(metadata, list):\n",
    "        for item in metadata:\n",
    "            for key, value in item.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(metadata)\n",
    "    print()\n",
    "\n",
    "    # Print the text\n",
    "    print(\"Text:\")\n",
    "    print(display_data[\"Text\"])\n",
    "    print()\n",
    "\n",
    "    # Print the embedding\n",
    "    print(\"Embedding:\")\n",
    "    print(display_data[\"Embedding\"])\n",
    "    print()\n",
    "\n",
    "# Example usage\n",
    "rec = 0  # Replace with the desired record number\n",
    "display_record(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed12f4d",
   "metadata": {},
   "source": [
    "## Update original documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "# Ensure 'text' column is of type string\n",
    "df['text'] = df['text'].astype(str)\n",
    "# Create documents with IDs\n",
    "documents = [Document(text=row['text'], doc_id=str(row['id'])) for _, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383ad27",
   "metadata": {},
   "source": [
    "## Knowledge Graph Index-based RAG\n",
    "\n",
    "### Generating the Knowledge Graph Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c274ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import KnowledgeGraphIndex, Settings\n",
    "\n",
    "# Using local models served by LM Studio\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "# Settings.llm = OpenAI(\n",
    "#     api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    "#     api_base=os.getenv(\"LM_STUDIO_API_BASE\"),\n",
    "#     model_name= os.getenv(\"LM_STUDIO_LLM_MODEL\"),\n",
    "# )\n",
    "\n",
    "\n",
    "# 使用智谱免费模型 glm-4-flash\n",
    "from llama_index.llms.zhipuai import ZhipuAI\n",
    "Settings.llm = ZhipuAI(\n",
    "    api_key=os.getenv(\"ZHIPU_API_KEY\"),\n",
    "    model=\"glm-4-flash\"\n",
    ")\n",
    "\n",
    "import time\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Graph index with embeddings\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-anything\"\n",
    "graph_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    max_triplets_per_chunk=2,\n",
    "    include_embeddings=True,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the execution time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Index creation time: {elapsed_time:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

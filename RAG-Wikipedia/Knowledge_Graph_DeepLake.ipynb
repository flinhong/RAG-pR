{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76e035",
   "metadata": {},
   "source": [
    "## Preparing the data for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69433ad3",
   "metadata": {},
   "source": [
    "### Pipeline 1: Collecting and preparing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567fef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name for file management\n",
    "graph_name = \"Marketing\"\n",
    "\n",
    "ufilename = graph_name + \"_urls.txt\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "with open(ufilename, 'r') as file:\n",
    "    urls = [line.strip() for line in file]\n",
    "\n",
    "print(\"Read URLs:\")\n",
    "for url in urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(content):\n",
    "    # Remove references and unwanted characters\n",
    "    content = re.sub(r'\\[\\d+\\]', '', content)   # Remove references\n",
    "    content = re.sub(r'[^\\w\\s\\.]', '', content)  # Remove punctuation (except periods)\n",
    "    return content\n",
    "\n",
    "def fetch_and_clean(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Prioritise \"mw-parser-output\" but fall back to \"content\" node if not found\n",
    "        content = soup.find('div', {'class': 'mw-parser-output'}) or soup.find('div', {'id': 'content'})\n",
    "        if content is None:\n",
    "            return None\n",
    "        \n",
    "        # Remove specific unwanted sections, including nested ones\n",
    "        for section_title in ['References', 'Bibliography', 'External links', 'See also', 'Notes']:\n",
    "            section = content.find('span', id=section_title)\n",
    "            while section:\n",
    "                for sib in section.parent.find_next_siblings():\n",
    "                    sib.decompose()  # Remove the section and its siblings\n",
    "                section.parent.decompose()  # Remove the section itself\n",
    "                section = content.find('span', id=section_title)\n",
    "\n",
    "        # Extract and clean text\n",
    "        text = content.get_text(separator=' ', strip=True)  # Use space as separator and strip whitespace\n",
    "        text = clean_text(text)\n",
    "        return text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None  # Return None if there's an error\n",
    "    \n",
    "# Directory to store the output file\n",
    "output_dir = './data/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Processing the URLs and skipping invalid ones\n",
    "reload = False  # Set to True to reprocess all URLs\n",
    "if reload==True:\n",
    "    for url in urls:\n",
    "        article_name = url.split('/')[-1].replace('.html', '')\n",
    "        filename = os.path.join(output_dir, f\"{article_name}.txt\")\n",
    "\n",
    "        clean_article_text = fetch_and_clean(url)\n",
    "        if clean_article_text:  # Only write if text is not None\n",
    "            with open(filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(clean_article_text)\n",
    "                print(f\"Saved {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a539dac",
   "metadata": {},
   "source": [
    "## Pipeline 2: Creating and populating the Deeplake Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fddd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup embedding model\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# loads https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "    embed_batch_size=10,  # default value (100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5244261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.core import StorageContext, VectorStoreIndex, Settings\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Path for vector store and dataset\n",
    "db = \"hub://honglin/marketing01\"\n",
    "vector_store_path = db\n",
    "dataset_path = db\n",
    "\n",
    "# Create an index over the documents\n",
    "# Overwrites the existing dataset if True\n",
    "ow = True\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=ow)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
